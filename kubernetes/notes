Taints and Tolerations
Tainting a node means no pod can be placed on that node unless it has been given a toleration in its definition.
There are 3 taint effects:
NoSchedule: no pods will be placed on the node
PreferedNoSchedule: it will try and avoid placing a pod on a node but its not guaranteed
NoExecute: no pods will be placed on teh node, and any existing pods will be removed

Just because a pod has a toleration, does not mean that pod will be scheduled on the node with a taint. It might be placed on a node that doesn't have a taint. If you need a pod to be on a particular node then this is the purpose of node affinity. 
Tains are automatically placed on master nodes when created to prevent any pods being placed on them by default

To taint a node = k taint node node01 spray=mortein:NoSchedule
To untaint a node = k taint node node01 spray:NoSchedule-
To setup a toleration in a yaml file:
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"

Affinity
To assign pods to nodes based on labels. Allows you to be more specific than nodeSelctor. The below example comes under the spec in the pod definition.
affinity:
    nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                          operator: In
                          values:
                         -  minikube

(lecture 64) If you have a scenario where you only want you pods on certain nodes and another team only want their pods on their own nodes, then you could use a both taint/tolerations and node affinity

Annotations
Similar to labels but these are not used to group items. They are there purely for any extra info you would like to place on the item, e.g. build_version, phone number, git sha etc

Manual scheduling:
in your pod.yaml file you could have something like 'nodeName: node1' under the pod spec. This will look for a node with that name

Node selector:
similar to manual scheduling above, only this time you have something similar to below, only this works off of labels on the node and not the node name
'nodeSelector:
  size: Large'

Resource limits (managed by the scheduler):
Kubernetes sets resource limits on containers; for CPU this default is 0.5CPU; for memory the default is 256Mi. However these defaults only apply if you have set a LimitRange in the namespace - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/

requests vs limits:
 requests: If you request 1Gi of memory and 1CPU, then it will look for a node that has these available. If no node can handle the request, the pods events will show an error that the node didn't have enough CPU/memory
 limits: in docker, containers can keep consuming resources on the node without ever hitting a limit and this can cause the node to crash. I kubernetes, you can limit this. You can set a limit for CPU and memory. CPU is blocked at the limit you set, but memory can still excede its limit. But if a pod is always trying to use more memory than its limit, the pod will be teminated

Daemon sets:
Runs one copy of your pod on each node in the cluster. Yaml file is almost identical to a RS but the kind is DaemonSet

Static pods:
if you had no api-server, scheduler or even master nodes you can still provision pods on worker nodes. To do this you would upload you yaml file directly on the node in '/etc/kubernetes/manifests. The kubelet periodically reads the files here and will create a pod if it sees a pod.yaml file or will destroy it if the file is removed. You can only do this with pods; not deployments/rs/services.

static pods have the node name appended to the end of the pod name. pods from the api-server pods have a random string at the end
(leacture 73) kubernetes can take care of both static pods and those created by the api-server. If a static pod is created a mirror copy will be sent to the api-server so you can see it when running a 'k get pods'; however you can't edit this as its a read-only copy. You would need to edit the manifest file

the file for the kubelet config is '/var/lib/kubelet/config.yaml'. This will tell you where all static pod manifests are stored under the key 'staticPodPath'. You may have static pods on different nodes, so its important to check what node a pod is on, then get onto that node and then find its manifest file if you need to make changes
