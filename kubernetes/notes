Taints and Tolerations
Tainting a node means no pod can be placed on that node unless it has been given a toleration in its definition.
There are 3 taint effects:
NoSchedule: no pods will be placed on the node
PreferedNoSchedule: it will try and avoid placing a pod on a node but its not guaranteed
NoExecute: no pods will be placed on teh node, and any existing pods will be removed

Just because a pod has a toleration, does not mean that pod will be scheduled on the node with a taint. It might be placed on a node that doesn't have a taint. If you need a pod to be on a particular node then this is the purpose of node affinity. 
Tains are automatically placed on master nodes when created to prevent any pods being placed on them by default

To taint a node = k taint node node01 spray=mortein:NoSchedule
To untaint a node = k taint node node01 spray:NoSchedule-
To setup a toleration in a yaml file:
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"

Affinity
To assign pods to nodes based on labels. Allows you to be more specific than nodeSelctor. The below example comes under the spec in the pod definition.
affinity:
    nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                          operator: In
                          values:
                         -  minikube

(lecture 64) If you have a scenario where you only want you pods on certain nodes and another team only want their pods on their own nodes, then you could use a both taint/tolerations and node affinity

Annotations
Similar to labels but these are not used to group items. They are there purely for any extra info you would like to place on the item, e.g. build_version, phone number, git sha etc

Manual scheduling:
in your pod.yaml file you could have something like 'nodeName: node1' under the pod spec. This will look for a node with that name

Node selector:
similar to manual scheduling above, only this time you have something similar to below, only this works off of labels on the node and not the node name
'nodeSelector:
  size: Large'

Resource limits (managed by the scheduler):
Kubernetes sets resource limits on containers; for CPU this default is 0.5CPU; for memory the default is 256Mi. However these defaults only apply if you have set a LimitRange in the namespace - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/

requests vs limits:
 requests: If you request 1Gi of memory and 1CPU, then it will look for a node that has these available. If no node can handle the request, the pods events will show an error that the node didn't have enough CPU/memory
 limits: in docker, containers can keep consuming resources on the node without ever hitting a limit and this can cause the node to crash. I kubernetes, you can limit this. You can set a limit for CPU and memory. CPU is blocked at the limit you set, but memory can still excede its limit. But if a pod is always trying to use more memory than its limit, the pod will be teminated

Daemon sets:
Runs one copy of your pod on each node in the cluster. Yaml file is almost identical to a RS but the kind is DaemonSet
