With regular volumes, in each pod def file, you need to create a vol and specify what size it should be. PV simplifies this
A PV is a cluster wide volume
Every PV is bound to a single PVC. During the binding process, k tries to find a PV that has sufficient capacity to satisfy the claim and any other request properties such as access mode, volume modes and storage class
If there are multiple matches, you can use labels (on the PV) and selectors (on the PVC) to define which claim should bind to which PV
Persistent volumes mean that you don't have to define a volume in each pod definition file. You create a persistent volume and pods get a section of that vol through a 'persistent volume claim'
A smaller PVC may attach to a larger PV if the other criteria match and there is no better option. Its a 1:1 relationship so no other claim can utilise the remaining space left on the larger PV
If there are no PV's available the PVC will remain in a pending state

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/data
  capacity:
    storage: 5Gi
OR

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  gcePersistentDisk:
    pdName: pd-disk
    fsType: ext4
  capacity:
    storage: 5Gi

This PVC only looks for 1Gi but the volume has 5Gi. Since there are no other volumes available, the PVC is bound to the PV 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


Once a PVC has been created you can then use this in a pod definiton file. The same can be done for deployments and replicasets
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: data-volume
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
